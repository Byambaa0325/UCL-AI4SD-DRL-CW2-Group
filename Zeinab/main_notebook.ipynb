{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2: Robotic behavioural cloning from images and actuator data\n",
    "\n",
    "NB: Please do **not** discuss the Coursework in the forum or any other public medium. Please ask for clarifications directly during Lab Sessions or at any time via an email directed to the TA (Eric Wanjau: eric.muriithi.22@ucl.ac.uk; Radu-Stefan Popovici: radu.popovici.25@ucl.ac.uk) or module lead. The module lead and the TAs will respond either via email or via a public announcement to all students. \n",
    "\n",
    "_You are required to submit 1 PDF report and 1 Python notebook. The PDF report should be a pdf version of the python notebook. Before converting to PDF ensureÂ all cells are excuted and all outputs are visible._    \n",
    "\n",
    "_This assignment is marked out of 100. A breakdown of how marks are allocated is included in the file._\n",
    "\n",
    "The filename for submission must be in the following format:  \n",
    "\n",
    "* _CandidateCode-CW2.pdf_  \n",
    "\n",
    "* _CandidateCode-CW2.ipynb_  \n",
    "\n",
    "and should be a completed version of the CW2_QUESTIONS.ipynb file. \n",
    "\n",
    "In total, there should be 2 files in submission.\n",
    "\n",
    "**Important: All group members must complete the \"Group Work Contribution Form\" below. This is a mandatory step. Penalties will be applied to groups where the form is incomplete or not accurately filled in. Please coordinate with your team to ensure this is done.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "<div style=\"background-color: #1b1f02ff; padding: 15px; border-radius: 5px; border-left: 4px solid #0b0104ff;\">\n",
    "\n",
    "## Group Work Contribution Form\n",
    "\n",
    "**Group Number:** Group 10\n",
    "\n",
    "\n",
    "## ðŸ“‹ Instructions for Completion\n",
    "\n",
    "1. **Individual Responsibility:** Each member must complete their own row in the contribution table below\n",
    "2. **Primary Ownership:** Each sub-question should be primarily solved by one member (though others may assist)\n",
    "3. **No Overlap:** Each question number can only be associated with one primary contributor\n",
    "4. **Be Specific:** Describe your contributions in detail - avoid vague statements like \"helped with coding\"\n",
    "5. **Be Honest:** Provide an accurate assessment of your work\n",
    "\n",
    "\n",
    "## ðŸ‘¥ Contribution Details [TODO]\n",
    "\n",
    "| Group Member | Student ID | Student Name | Question Numbers You Primarily Contribute | Other Specific Contributions (Maximum 100 words per member) |\n",
    "|--------------|------------|--------------|-------------------------------------------|------------------------------------------------------------|\n",
    "| **Member 1** | `[1234567]` | `[Alex Kim]` | `[1.a, 1.b.i, 1.c.ii.iii]` | `Describe your other contributions in this group coursework that you want to highlight, e.g., â€¢ Developed the initial project timeline; â€¢ Solved challenging bug in Question 1.b.i; â€¢ Coordinated weekly group meetings and tracked progress; â€¢ Created data visualization figures for the justificatin;â€¢ Use some very innovative methods or solutions, etc., You can put anything that you think is useful to this group work here`|\n",
    "| **Member 2** | `[Your ID]` | `[Your Name]` | `[Your solved question numbers]` | `â€¢ [Your Other specific contributions here]` |\n",
    "| **Member 3** | `[ ]` | `[ ]` | `[ ]` | `â€¢ [Your other specific contributions here]` |\n",
    "| **Member 4** | `[ ]` | `[ ]` | `[ ]` | `â€¢ [Your other specific contributions here]` |\n",
    "| **Member 5** | `[ ]` | `[ ]` | `[ ]` | `â€¢ [Your other specific contributions here]` |\n",
    "\n",
    "\n",
    "## ðŸ“Š Question Number Reference\n",
    "\n",
    "### Available Questions for Primary Contribution:\n",
    "**Question 1**  \n",
    "â€¢ 1.a  \n",
    "â€¢ 1.b.i  \n",
    "â€¢ 1.b.ii.i  \n",
    "â€¢ 1.b.ii.ii  \n",
    "â€¢ 1.b.ii.iii  \n",
    "â€¢ 1.c.i  \n",
    "â€¢ 1.c.ii.i  \n",
    "â€¢ 1.c.ii.ii  \n",
    "\n",
    "**Question 2**  \n",
    "â€¢ 2.a  \n",
    "â€¢ 2.b  \n",
    "â€¢ 2.c  \n",
    "\n",
    "**Question 3**  \n",
    "â€¢ 3.a  \n",
    "â€¢ 3.b.i  \n",
    "â€¢ 3.b.ii  \n",
    "\n",
    "\n",
    "## âœ… Group Declaration\n",
    "\n",
    "We confirm that:\n",
    "\n",
    "- We have read and understood the contribution descriptions above\n",
    "- We believe this to be a fair and accurate representation of work distribution\n",
    "- Each question has one primary contributor as indicated\n",
    "- All members have contributed significantly to the final submitted work\n",
    "\n",
    "\n",
    "## ðŸ’¡ Notes\n",
    "\n",
    "- Ensure question numbers are correctly formatted (e.g., \"1.a\", \"1.b.ii.i\")\n",
    "- Keep contribution descriptions concise but specific (max 100 words per member)\n",
    "- Contact your instructor if there are any concerns about contribution fairness\n",
    "\n",
    "</div>\n",
    "\n",
    "---  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "For this coursework, you are required to develop a behaviour cloning model. Behaviour cloning is a technique for training robotic agents using a dataset of sequences of actions. Consider the supervised learning definition where we have a dataset of observatios $d=\\{(x_{1},y_{1}),...,(x_{n},y_{n})\\}$ and the aim is to learn a function: $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$. In this case, $\\mathcal{X}$ is the set of \"observations\" that the robot makes and $\\mathcal{Y}$ is the set of actions that the robot takes.\n",
    "\n",
    "The dataset you have been provided with contains examples of robot arms being required to pickup objects or drop objects (given it has already picked the object up) in a specific place. The observation space ($\\mathcal{X}$) consists of:\n",
    "- \"front_cam_ob\": A 3rd person image of the scene \n",
    "- \"mount_cam_ob\": An image of the scene taken from a camera mounted on top of the robotic arm, looking down\n",
    "- \"ee_cartesian_pos_ob\": The positional and orientation co-ordinates of the robotic arm\n",
    "- \"ee_cartesian_vel_ob\": The velocity of position and orientation of the robotic arm\n",
    "- \"joint_pos_ob\": The position of the gripper which opens and closes\n",
    "\n",
    "The action space ($\\mathcal{Y}$) consists of:\n",
    "- Three co-ordinates defining how much to move the robotic arm\n",
    "- An action defining whether to open, not to move or close the gripper\n",
    "\n",
    "The dataset is split into \"trajectories\" i.e., sequences of:\n",
    "- $x_{i}$: The front_cam_ob, mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob, joint_pos_ob __at time point i__ \n",
    "- $y_{i}$: The action taken i.e., how to move the arm and the gripper __given the observations__ in $x_{i}$\n",
    "\n",
    "More information on the dataset can be found at: https://github.com/clvrai/clvr_jaco_play_dataset?tab=readme-ov-file\n",
    "\n",
    "### Task\n",
    "Your task has been split into several questions, each exploring how to develop an appropriate model for learning $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$. The task will require you to:\n",
    "- Question 1: Tune an end-to-end supervised learning model taking in the full set of observations and predicting actions: You will be required to evaluate a proposed deep learning architecture (which takes as input all of front_cam_ob, mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob and joint_pos_ob and predict the two actions) and propose a new model which outperforms the existing model;\n",
    "- Question 2: Define and evaluate a VAE model for performing self-supervised learning and tune it as best you can, to learn a latent representation that can be used as input to a downstream supervised model for behaviour cloning\n",
    "- Question 3: Evaluate the performance of your model proposed in question 1 against your self-supervised VAE representations from question 2 (plus a supervised head) on the test set\n",
    "\n",
    "### Pointers\n",
    "Some helper functions have been provided for you including the following functionality:\n",
    "- A training and validation loop capabale of:\n",
    "  - Handling \"half-precision\" modelling;\n",
    "  - Logging results to weights and biases;\n",
    "- An eda template to help you visualise the data\n",
    "- An evaluation template to help you load saved model checkpoints from weights and biases\n",
    "- A preprocessing script to help you convert the data into train/validation and test splits;\n",
    "  - In this preprocessing script, trajectories longer than 75 timesteps have been removed to ease the computational requirements of the task;\n",
    "- A torch Dataset class capable of handling the multi-model nature of the data;\n",
    "- A example collate_fn to use in Dataloaders\n",
    "\n",
    "Additionally, it is strongly suggested to call ```torch.manual_seed(1)``` whenever you initialise your model (i.e., when you first create the model or call model.reset()). This will ensure the parameters are initialised at the same value each time.\n",
    "\n",
    "### IMPORTANT\n",
    "- You are __not__ allowed to use pre-trained models, developed outside of this coursework i.e., you could __not__ use a pre-trained YOLO model\n",
    "- Questions have been marked under the title \"Task\", ensure that you answer/address all of the bullet points under these headings\n",
    "- Pls add proper citations or references where necessary in your justification\n",
    "- You can use markdown language to add formatting to your text. A cheatsheet is found [here](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)\n",
    "- The completed assignment jupyter notebook (With Python code and English Text) should have all the output cells been executed and solution outputs are visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "Download the data for the project from here: ~~https://drive.google.com/file/d/1ss_1RZZzov2SdJsYI_mRTpo6VBMRK43O/view?usp=sharing~~\n",
    "\n",
    "https://drive.google.com/drive/folders/1QbiqeqXuVh2iLOOhPg1ztTXEnh1cVput\n",
    "\n",
    "Save and unzip the data locally at: ../data/all_play_data_diverse or in Google Collab at: /content/drive/MyDrive/comp0188_2425/cw2. Saving the data in these locations will ensure the proprocessing script provided runs correctly. If you would like to alter these locations, you can alter them in the config.py file of the provided comp0188_cw2 package via the ROOT_PATH global variable.\n",
    "\n",
    "### transition_df.csv\n",
    "You have additionally been provided with a csv called \"transition_df.csv\". This contains a row for each observation/action pair in the dataset and is used to generate the train/validation and test datasets for this task. Note that this csv contains all trajectories (even those over 75 timesteps). This csv might also be useful for EDA. The transition_df.csv should be placed in the same folder that you placed the raw data in (discussed above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comp0188_cw2 contains some config functionality so that you can run the code in collab and locally (to test models)! When you first import the package, import the project_options and set the appropriate configs.\n",
    "- project_options.collab = True will set the dataset directories for google collab whilst false will set suitable local directories\n",
    "- project_options.debug = True will load a subset of data whilst False will load all of the data. \n",
    "\n",
    "__IMPORTANT__: Alterting these options __once you have loaded other functionality__ from comp0188_cw2 may result in unintended outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLAB = False\n",
    "if COLLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #!pip install --force-reinstall git+https://github.com/joshuaspear/pymlrf.git``\n",
    "    #!pip install wandb\n",
    "    #!pip install torchinfo\n",
    "    #!pip install jaxtyping\n",
    "    #!pip install git+https://github.com/joshuaspear/comp0188_cw2_public.git\n",
    "    #!pip install typeguard==2.13.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEAM**: If you want to run locally, connect to a conda or venv kernel and run following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymlrf@ git+https://github.com/joshuaspear/pymlrf.git (from -r ../requirements.txt (line 14))\n",
      "  Cloning https://github.com/joshuaspear/pymlrf.git to c:\\users\\byamb\\appdata\\local\\temp\\pip-install-lposgt0f\\pymlrf_11d15366584047b289dcf22768e6a063\n",
      "  Resolved https://github.com/joshuaspear/pymlrf.git to commit 13f5c1cbad249ec6d5a6cd356612c874ae68bb53\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting comp0188_cw2@ git+https://github.com/joshuaspear/comp0188_cw2_public.git (from -r ../requirements.txt (line 20))\n",
      "  Cloning https://github.com/joshuaspear/comp0188_cw2_public.git to c:\\users\\byamb\\appdata\\local\\temp\\pip-install-lposgt0f\\comp0188-cw2_5a9e52d4181f473d8837070bad53d3e4\n",
      "  Resolved https://github.com/joshuaspear/comp0188_cw2_public.git to commit 5057522998ce2732f354cd1bc232908c823f6960\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 5)) (2.9.1)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 6)) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 7)) (2.4.0)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: jaxtyping in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 11)) (0.3.4)\n",
      "Requirement already satisfied: wandb in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 17)) (0.23.1)\n",
      "Requirement already satisfied: typeguard==2.13.3 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 23)) (2.13.3)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 26)) (2.3.3)\n",
      "Requirement already satisfied: h5py>=3.7.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 27)) (3.15.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 30)) (3.10.8)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 31)) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 34)) (1.8.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from -r ../requirements.txt (line 35)) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torch>=2.0.0->-r ../requirements.txt (line 5)) (80.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from torchvision>=0.15.0->-r ../requirements.txt (line 6)) (12.0.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from jaxtyping->-r ../requirements.txt (line 11)) (0.1.7)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (3.1.45)\n",
      "Requirement already satisfied: packaging in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (25.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (6.33.2)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (2.12.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from wandb->-r ../requirements.txt (line 17)) (2.48.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 17)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 17)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 17)) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 17)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 17)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 17)) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 17)) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from pandas>=1.5.0->-r ../requirements.txt (line 26)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from pandas>=1.5.0->-r ../requirements.txt (line 26)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from pandas>=1.5.0->-r ../requirements.txt (line 26)) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from matplotlib>=3.6.0->-r ../requirements.txt (line 30)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from matplotlib>=3.6.0->-r ../requirements.txt (line 30)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from matplotlib>=3.6.0->-r ../requirements.txt (line 30)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from matplotlib>=3.6.0->-r ../requirements.txt (line 30)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from matplotlib>=3.6.0->-r ../requirements.txt (line 30)) (3.2.5)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from scikit-learn>=1.2.0->-r ../requirements.txt (line 34)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from scikit-learn>=1.2.0->-r ../requirements.txt (line 34)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from scikit-learn>=1.2.0->-r ../requirements.txt (line 34)) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from tqdm>=4.65.0->-r ../requirements.txt (line 35)) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 17)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 17)) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->-r ../requirements.txt (line 26)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\byamb\\miniconda3\\envs\\coursework2\\lib\\site-packages (from jinja2->torch>=2.0.0->-r ../requirements.txt (line 5)) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/joshuaspear/pymlrf.git 'C:\\Users\\byamb\\AppData\\Local\\Temp\\pip-install-lposgt0f\\pymlrf_11d15366584047b289dcf22768e6a063'\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/joshuaspear/comp0188_cw2_public.git 'C:\\Users\\byamb\\AppData\\Local\\Temp\\pip-install-lposgt0f\\comp0188-cw2_5a9e52d4181f473d8837070bad53d3e4'\n"
     ]
    }
   ],
   "source": [
    "LOCAL = True\n",
    "if LOCAL:\n",
    "    %pip install -r ../requirements.txt\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the error: \n",
    "```ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.inflect 7.4.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from comp0188_cw2 import project_options\n",
    "project_options.collab = COLLAB\n",
    "print(project_options.collab)\n",
    "project_options.debug = True\n",
    "print(project_options.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20m2025-12-20 23:05:34,730 - pymlrf - WARNING - pymlrf environment variable not set. Logging to file will not be performed (__init__.py:50)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from pymlrf.Structs.torch import DatasetOutput\n",
    "import copy\n",
    "\n",
    "from comp0188_cw2.utils import load_all_files\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "from typing import List\n",
    "from pymlrf.Structs.torch import DatasetOutput\n",
    "\n",
    "from comp0188_cw2.config import (\n",
    "    train_dh, val_dh, test_dh, WANDB_PROJECT\n",
    "    )\n",
    "from comp0188_cw2.models.CNNConfig import ConvolutionLayersConfig\n",
    "from comp0188_cw2.models.base import BaseModel\n",
    "from comp0188_cw2.models.JointCNNEncoder import JointCNNEncoder\n",
    "from comp0188_cw2.models.CNN import CNN\n",
    "from comp0188_cw2.models.MLP import MLP\n",
    "from comp0188_cw2.Metric.WandBMetricOrchestrator import WandBMetricOrchestrator\n",
    "from comp0188_cw2.Dataset.NpDictDataset import NpDictDataset\n",
    "from comp0188_cw2.Loss.BalancedLoss import TrackerBalancedLoss\n",
    "from comp0188_cw2 import logger\n",
    "from comp0188_cw2.training.TrainingLoop import TorchTrainingLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is workspace\n",
      " Volume Serial Number is 9676-681F\n",
      "\n",
      " Directory of e:\\UCL-Workspaces\\deep-representations\\CourseWork2\\UCL-AI4SD-DRL-CW2-Group\\data\\all_play_data_diverse\n",
      "\n",
      "12/20/2025  10:36 PM    <DIR>          .\n",
      "12/20/2025  10:36 PM    <DIR>          ..\n",
      "12/20/2025  09:38 PM                 0 .gitkeep\n",
      "12/20/2025  09:13 PM    23,491,950,434 all_play_data_diverse.h5\n",
      "12/20/2025  09:44 PM    <DIR>          debug\n",
      "12/20/2025  10:38 PM    <DIR>          test\n",
      "12/20/2025  10:38 PM    <DIR>          train\n",
      "12/20/2025  12:11 AM         1,026,147 transition_df.csv\n",
      "12/20/2025  10:38 PM    <DIR>          val\n",
      "               3 File(s) 23,492,976,581 bytes\n",
      "               6 Dir(s)  33,275,437,056 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir ..\\data\\all_play_data_diverse\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/all_play_data_diverse\\debug\\train\n"
     ]
    }
   ],
   "source": [
    "print(train_dh.loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "You will only need to perform this step __once__ for the full dataset and __once__ for the debug dataset for the entire coursework, both locally and in Google collab. In Google Collab, the data will be saved in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debug datasets\n",
      "(35722, 5)\n",
      "Transitions excluded: 0.5418200474571924\n",
      "25469\n",
      "2901\n",
      "7352\n",
      "453\n",
      "51\n",
      "126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\Preprocessing.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tran_df_sub[\"is_test\"] = np.where(\n",
      "c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\Preprocessing.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tran_df_sub[\"is_val\"] = np.where(\n",
      "c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\Preprocessing.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tran_df_sub[\"is_train\"] = (1-tran_df_sub[[\"is_test\",\"is_val\"]].max(axis=1))\n"
     ]
    }
   ],
   "source": [
    "from comp0188_cw2.Preprocessing import main\n",
    "RUN_PREPROCESSING = True\n",
    "if RUN_PREPROCESSING:\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{Red}{Question\\ 1}:$ Tune an end-to-end supervised learning model taking in the full set of observations and predicting actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.11it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.80it/s]\n"
     ]
    }
   ],
   "source": [
    "_keys = [\n",
    "    \"actions\",\"front_cam_ob\",\n",
    "    \"mount_cam_ob\",\"terminals\",\n",
    "    \"ee_cartesian_pos_ob\",\n",
    "    \"ee_cartesian_vel_ob\",\n",
    "    \"joint_pos_ob\"\n",
    "    ]\n",
    "train_nps = load_all_files(train_dh.loc,\"train_[0-9]+.h5\",keys=_keys)\n",
    "val_nps = load_all_files(val_dh.loc,\"val_[0-9]+.h5\",keys=_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{Red}{Question\\ 1.a}$ Preprocessing steps and Justification (5 Marks)\n",
    "Most likely in machine learning pipelines, input data needs to be preprocessed before passing it to the model. This question requires you to specify the preprocessing that you will perform for the different types of data i.e., front_cam_ob, mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob and joint_pos_ob. The dataset class provided in the associated \"comp0188_cw2\" package enables you to pass a dictionary of functions to preprocess each element of the observations and actions. The class expects a dictionary of transformations to apply to each input/output. \n",
    "\n",
    "##### $\\color{Red}{Task-1}$ (2 Marks)\n",
    "- Complete the dictionaries below, specifying the type of transformations you wish to perform. For each element (of the observations and actions), you should __at least__ convert the output to a tensor thus, these transformations have been implemented for you. You may alter __any__ part of the code between the \"INSERT YOUR CODE HERE\" comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 50 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "global_transforms = {\n",
    "    \"front_cam_ob\":\n",
    "        transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ]),\n",
    "    \"mount_cam_ob\": transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            ]),\n",
    "    \"actions\": lambda x: torch.tensor(x),\n",
    "    \"ee_cartesian_pos_ob\": lambda x: torch.tensor(x),\n",
    "    \"ee_cartesian_vel_ob\": lambda x: torch.tensor(x),\n",
    "    \"joint_pos_ob\": lambda x: torch.tensor(x)\n",
    "}\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### $\\color{Red}{Task-2}$ (3 Marks)\n",
    "- For the below variables, justify your decisions for preprocessing including where you have decided __not__ to apply preprocessing. You should include empirical evidence from your EDA analysis to support your decisions. Justfication __without__ evidence will be rewarded 0 marks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_actions_\n",
    "\n",
    "\n",
    "_front_cam_ob_\n",
    "\n",
    "\n",
    "_mount_cam_ob_\n",
    "\n",
    "\n",
    "_ee_cartesian_pos_ob_\n",
    "\n",
    "\n",
    "_ee_cartesian_vel_ob_\n",
    "\n",
    "\n",
    "_joint_pos_ob_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 600 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### $\\color{Red}{Question\\ 1.b}$  End-to-end supervised model evaluation\n",
    "The code below defines an end to end supervised model which: \n",
    "- Jointly encodes the two images (\"front_cam_ob\", \"mount_cam_ob\") using a CNN architecture (image_encoder);\n",
    "- Seperately encoding the positional and velocity observations using an MLP;\n",
    "- Combines the two embeddings by adding them together and;\n",
    "- Passes the combined embedding into a final MLP layer (dense)\n",
    "  \n",
    "This question requires you to define sutable loss functions for the model and then evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2_config = ConvolutionLayersConfig(\n",
    "    input_dim=224,\n",
    "    input_channels=2,\n",
    "    layers=[\n",
    "        nn.Conv2d(\n",
    "            in_channels=2,\n",
    "            out_channels=8,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            dilation=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2)),\n",
    "        nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=(3,3),\n",
    "            stride=1,\n",
    "            padding=1\n",
    "          ),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=(2,2))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_cnn_output_dim = cnn2_config.get_output_dims()\n",
    "_cnn_output_channels = cnn2_config.get_output_channels()\n",
    "_mlp_input_dim = int(\n",
    "            (_cnn_output_dim[-1]*_cnn_output_dim[-1])*_cnn_output_channels[-1]\n",
    "            )\n",
    "\n",
    "\n",
    "image_encoder = JointCNNEncoder(\n",
    "    cnn=CNN(cnn2_config),\n",
    "    dense=MLP(\n",
    "        input_dim=_mlp_input_dim,\n",
    "        hidden_dims=[256],\n",
    "        output_dim=128\n",
    "        )\n",
    ")\n",
    "\n",
    "obs_encoder = MLP(\n",
    "    input_dim = 15,\n",
    "    hidden_dims = [256,256],\n",
    "    output_dim = 128\n",
    ")\n",
    "\n",
    "dense = MLP(\n",
    "    input_dim = 128,\n",
    "    hidden_dims = [64,32],\n",
    "    output_dim = 6\n",
    ")\n",
    "\n",
    "class Baseline1(BaseModel):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      image_encoder:JointCNNEncoder,\n",
    "      obs_encoder:MLP,\n",
    "      dense:MLP\n",
    "      ) -> None:\n",
    "      super().__init__()\n",
    "      self.image_encoder = image_encoder\n",
    "      self.obs_encoder = obs_encoder\n",
    "      self.dense = dense\n",
    "\n",
    "  def forward(self, images, obs):\n",
    "    _img_enc = self.image_encoder(images)\n",
    "    _obs_enc = self.obs_encoder(obs)\n",
    "    _dense_enc = self.dense(_img_enc+_obs_enc)\n",
    "    pos = _dense_enc[:,0:3]\n",
    "    grp = _dense_enc[:,3:]\n",
    "    return {\n",
    "        \"pos\": pos,\n",
    "        \"grp\":grp\n",
    "        }\n",
    "  def reset(\n",
    "      self,\n",
    "      image_encoder_kwargs,\n",
    "      obs_encoder_kwargs,\n",
    "      dense_kwargs\n",
    "      ):\n",
    "    self.image_encoder.reset(**image_encoder_kwargs)\n",
    "    self.obs_encoder.reset(**obs_encoder_kwargs)\n",
    "    self.dense.reset(**dense_kwargs)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = Baseline1(\n",
    "    image_encoder=image_encoder,\n",
    "    obs_encoder=obs_encoder,\n",
    "    dense=dense\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{Red}{Question\\ 1.b.i}$ Loss definitions (3 marks)\n",
    "For the model defined above, the proposed loss function is defined where the contribution of \"pos_criterion\" and \"grp_criterion\" are equally weighted and the mean of the two values loss are used to define the final loss. Furthermore, the loss for the positional actions is the MSE and the loss for grp_criterion is the CrossEntropyLoss.\n",
    "\n",
    "##### Task:\n",
    "- Justify why this composite loss function is reasonable. You should make reference to the range of values predicted by the deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 250 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_criterion = nn.MSELoss(reduction=\"mean\")\n",
    "grp_criterion = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n",
      "torch.Size([4, 2, 224, 224])\n",
      "torch.Size([4, 15])\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n",
      "tensor(1126.0417)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model.reset({},{},{})\n",
    "exp_kwargs = {\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"half_precision\": True,\n",
    "    \"target_offset\": 0\n",
    "}\n",
    "\n",
    "if project_options.debug:\n",
    "    exp_kwargs[\"batch_size\"] = 4\n",
    "\n",
    "if exp_kwargs[\"half_precision\"]:\n",
    "    train_dataset = NpDictDataset(\n",
    "        array_dict=train_nps,\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "\n",
    "    val_dataset = NpDictDataset(\n",
    "        array_dict=val_nps,\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "else:\n",
    "    train_dataset = NpDictDataset(\n",
    "        array_dict={k:train_nps[k].astype(np.float32) for k in train_nps},\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "\n",
    "    val_dataset = NpDictDataset(\n",
    "        array_dict={k:val_nps[k].astype(np.float32) for k in val_nps},\n",
    "        transform_lkp = global_transforms,\n",
    "        dep_vars = [\"actions\"],\n",
    "        indep_vars = [\n",
    "            \"front_cam_ob\",\"mount_cam_ob\", \"ee_cartesian_pos_ob\",\n",
    "            \"ee_cartesian_vel_ob\", \"joint_pos_ob\"\n",
    "            ],\n",
    "        target_offset=exp_kwargs[\"target_offset\"]\n",
    "        )\n",
    "\n",
    "print(len(train_dataset))\n",
    "out = train_dataset[0]\n",
    "\n",
    "def collate_func(input_list:List[DatasetOutput])->DatasetOutput:\n",
    "    pos = []\n",
    "    _grp = []\n",
    "    images = []\n",
    "    obs = []\n",
    "    for val in input_list:\n",
    "        images.append(\n",
    "            torch.concat(\n",
    "                [val.input[\"front_cam_ob\"], val.input[\"mount_cam_ob\"]],\n",
    "                dim=0\n",
    "            )[None,:]\n",
    "            )\n",
    "        obs.append(\n",
    "            torch.concat(\n",
    "                [\n",
    "                    val.input[\"ee_cartesian_pos_ob\"],\n",
    "                    val.input[\"ee_cartesian_vel_ob\"],\n",
    "                    val.input[\"joint_pos_ob\"]\n",
    "                    ],\n",
    "                dim=0\n",
    "            )[None,:]\n",
    "        )\n",
    "        pos.append(val.output[\"actions\"][0:3][None,:])\n",
    "        _grp.append(val.output[\"actions\"][-1:][None])\n",
    "    _grp = torch.concat(_grp, dim=0)\n",
    "    grp = torch.zeros(_grp.shape[0],3)\n",
    "    grp[torch.arange(len(grp)), _grp.squeeze().int()] = 1\n",
    "    return DatasetOutput(\n",
    "        input = {\n",
    "            \"images\":torch.concat(images,dim=0),\n",
    "            \"obs\":torch.concat(obs,dim=0),\n",
    "            },\n",
    "        output = {\n",
    "            \"pos\":torch.concat(pos, dim=0),\n",
    "            \"grp\":grp\n",
    "            }\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=exp_kwargs[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_func,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=exp_kwargs[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_func,\n",
    ")\n",
    "\n",
    "first_batch = next(train_dataloader.__iter__())\n",
    "\n",
    "input_dim = first_batch.input[\"images\"].shape\n",
    "print(input_dim)\n",
    "input_dim = first_batch.input[\"obs\"].shape\n",
    "print(input_dim)\n",
    "pos_dim = first_batch.output[\"pos\"].shape\n",
    "print(pos_dim)\n",
    "grp_dim = first_batch.output[\"grp\"].shape\n",
    "print(grp_dim)\n",
    "\n",
    "exp_kwargs[\"model_def\"] = model.__repr__()\n",
    "\n",
    "\n",
    "if exp_kwargs[\"half_precision\"]:\n",
    "    model = model.half()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=exp_kwargs[\"learning_rate\"],\n",
    "    eps=1e-04\n",
    "    )\n",
    "\n",
    "__criterion = TrackerBalancedLoss(\n",
    "    loss_lkp={\n",
    "        \"pos\":pos_criterion,\n",
    "        \"grp\": grp_criterion\n",
    "    }\n",
    "    )\n",
    "\n",
    "if exp_kwargs[\"half_precision\"]:\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                res = model(\n",
    "                    images=first_batch.input[\"images\"].cuda(),\n",
    "                    obs=first_batch.input[\"obs\"].cuda()\n",
    "                    )\n",
    "                first_batch.output[\"pos\"] = first_batch.output[\"pos\"].cuda()\n",
    "                first_batch.output[\"grp\"] = first_batch.output[\"grp\"].cuda()\n",
    "        else:\n",
    "            with torch.autocast(device_type=\"cpu\"):\n",
    "                res = model(\n",
    "                    images=first_batch.input[\"images\"],\n",
    "                    obs=first_batch.input[\"obs\"]\n",
    "                    )\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            res = model(\n",
    "                images=first_batch.input[\"images\"].cuda(),\n",
    "                obs=first_batch.input[\"obs\"].cuda()\n",
    "                )\n",
    "            first_batch.output[\"pos\"] = first_batch.output[\"pos\"].cuda()\n",
    "            first_batch.output[\"grp\"] = first_batch.output[\"grp\"].cuda()\n",
    "        else:\n",
    "            res = model(\n",
    "                images=first_batch.input[\"images\"],\n",
    "                obs=first_batch.input[\"obs\"]\n",
    "                )\n",
    "print(__criterion(res,first_batch.output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA device not available\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbyambaa-bayarmandakh-25\u001b[0m (\u001b[33mbyambaa-bayarmandakh-25-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\UCL-Workspaces\\deep-representations\\CourseWork2\\UCL-AI4SD-DRL-CW2-Group\\notebooks\\wandb\\run-20251220_230546-4rq5axdl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/byambaa-bayarmandakh-25-ucl/cw2_v2/runs/4rq5axdl' target=\"_blank\">End_to_end_1_DEBUG</a></strong> to <a href='https://wandb.ai/byambaa-bayarmandakh-25-ucl/cw2_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/byambaa-bayarmandakh-25-ucl/cw2_v2' target=\"_blank\">https://wandb.ai/byambaa-bayarmandakh-25-ucl/cw2_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/byambaa-bayarmandakh-25-ucl/cw2_v2/runs/4rq5axdl' target=\"_blank\">https://wandb.ai/byambaa-bayarmandakh-25-ucl/cw2_v2/runs/4rq5axdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/319 [00:00<01:35,  3.30it/s]c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\autograd\\graph.py:841: UserWarning: Error detected in LogSoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\asyncio\\base_events.py\", line 677, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\asyncio\\base_events.py\", line 2046, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\asyncio\\events.py\", line 94, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 602, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\byamb\\AppData\\Local\\Temp\\ipykernel_83064\\3922674570.py\", line 36, in <module>\n",
      "    sl_trainer.training_loop(\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\TrainingLoop.py\", line 281, in training_loop\n",
      "    mo, epoch = train(\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\TrainingLoop.py\", line 107, in train\n",
      "    train_loss_val, train_preds =  train_epoch_func(\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\train_single_epoch.py\", line 100, in __call__\n",
      "    train_loss = criterion(output, output_vals)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\Loss\\BalancedLoss.py\", line 52, in __call__\n",
      "    _loss = self.loss_lkp[key](pred[key], act[key])\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 1385, in forward\n",
      "    return F.cross_entropy(\n",
      "  File \"c:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\nn\\functional.py\", line 3458, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:127.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  1%|          | 2/319 [00:00<02:20,  2.26it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward0' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     wandb_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwandb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_DEBUG\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m orig = datetime.datetime.now()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43msl_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_proj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWANDB_PROJECT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_grp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb_grp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_encoder_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobs_encoder_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdense_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m post_train = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\TrainingLoop.py:281\u001b[39m, in \u001b[36mTorchTrainingLoop.training_loop\u001b[39m\u001b[34m(self, train_loader, val_loader, wandb_proj, wandb_config, wandb_name, wandb_grp, reset_kwargs, reset_model)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chkpnt_dh.is_created:\n\u001b[32m    280\u001b[39m     chkpnt_dh.create()\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m mo, epoch = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m  \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m  \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m  \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m  \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchkpnt_dh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m  \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m  \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmo\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m  \u001b[49m\u001b[43mval_criterion\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtrain_epoch_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainSingleEpoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhalf_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhalf_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_preds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_preds\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m  \u001b[49m\u001b[43mval_epoch_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mValidateSingleEpoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhalf_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhalf_precision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_preds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_preds\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpreds_save_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreds_save_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m  \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_dir\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m chckpnt_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(chkpnt_dh.loc) \u001b[38;5;28;01mif\u001b[39;00m f[-\u001b[32m3\u001b[39m:]==\u001b[33m\"\u001b[39m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m chckpnt_files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\TrainingLoop.py:107\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_data_loader, val_data_loader, gpu, optimizer, criterion, epochs, logger, save_dir, scheduler, train_epoch_func, val_epoch_func, seed, mo, val_criterion, preds_save_type, output_dir)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m np.arange(\u001b[32m1\u001b[39m,epochs+\u001b[32m1\u001b[39m):\n\u001b[32m    106\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mRunning training epoch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     train_loss_val, train_preds =  \u001b[43mtrain_epoch_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     epoch_train_loss = train_loss_val.numpy()\n\u001b[32m    112\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m training loss : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    113\u001b[39m             epoch, epoch_train_loss))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\train_single_epoch.py:118\u001b[39m, in \u001b[36mTrainSingleEpoch.__call__\u001b[39m\u001b[34m(self, model, data_loader, gpu, optimizer, criterion, logger)\u001b[39m\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    117\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mRuntime error on training instance: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(i))\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    119\u001b[39m _prd_lst = {}\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_preds:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\comp0188_cw2\\training\\train_single_epoch.py:113\u001b[39m, in \u001b[36mTrainSingleEpoch.__call__\u001b[39m\u001b[34m(self, model, data_loader, gpu, optimizer, criterion, logger)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# losses.update(train_loss.data[0], g.size(0))\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# error_ratio.update(evaluation(output, target).data[0], g.size(0))\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# compute gradient and do SGD step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[43mtrain_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     optimizer.step()\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\byamb\\miniconda3\\envs\\coursework2\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Function 'LogSoftmaxBackward0' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "mo = WandBMetricOrchestrator()\n",
    "\n",
    "train_criterion = TrackerBalancedLoss(\n",
    "    loss_lkp={\n",
    "        \"pos\":copy.deepcopy(pos_criterion),\n",
    "        \"grp\":copy.deepcopy(grp_criterion)\n",
    "    },\n",
    "    name=\"train\",\n",
    "    mo=mo\n",
    "    )\n",
    "\n",
    "val_criterion = TrackerBalancedLoss(\n",
    "    loss_lkp={\n",
    "        \"pos\":copy.deepcopy(pos_criterion),\n",
    "        \"grp\":copy.deepcopy(grp_criterion)\n",
    "    },\n",
    "    name=\"val\",\n",
    "    mo=mo\n",
    "    )\n",
    "\n",
    "sl_trainer = TorchTrainingLoop(\n",
    "    model=model, gpu=True, optimizer=optimizer, criterion=train_criterion,\n",
    "    val_criterion=val_criterion, epochs=10, logger=logger,\n",
    "    mo=WandBMetricOrchestrator(), half_precision=exp_kwargs[\"half_precision\"],\n",
    "    preds_save_type=None\n",
    ")\n",
    "\n",
    "wandb_name = \"End_to_end_1\"\n",
    "wandb_grp=\"End_to_end\"\n",
    "\n",
    "if project_options.debug:\n",
    "    wandb_name = f\"{wandb_name}_DEBUG\"\n",
    "\n",
    "\n",
    "orig = datetime.datetime.now()\n",
    "sl_trainer.training_loop(\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    wandb_proj=WANDB_PROJECT,\n",
    "    wandb_grp=wandb_grp,\n",
    "    wandb_config=exp_kwargs,\n",
    "    wandb_name=wandb_name,\n",
    "    reset_kwargs={\n",
    "        \"image_encoder_kwargs\": {},\n",
    "        \"obs_encoder_kwargs\": {},\n",
    "        \"dense_kwargs\": {}\n",
    "    }\n",
    "    )\n",
    "post_train = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 1.b.ii}$ Model evaluation (marks broken down in sub questions)\n",
    "This question requires you to evaluate the performance of the model by analyzing both overall metrics and specific failure patterns. Run the cells above which will train the model for 10 epochs.\n",
    "\n",
    "**IMPORTANT, for questions (1.b.ii.i, 1.b.ii.ii, 1.b.ii.iii):**\n",
    "- You are **not** expected to train a baseline model and will be awarded **0 marks** for doing so\n",
    "- **Incorrect interpretations of metrics** will be **negatively penalised**\n",
    "- Where you are asked for empirical evidence but provide none, you will be awarded **0 marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{Red}{Question\\ 1.b.ii.i}$ Epoch selection (3 marks) \n",
    "##### Task\n",
    "- Select an appropriate epoch using a suitable method and provide **empirical evidence** for your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 200 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 1.b.ii.ii}$ Holistic Performance Analysis (8 marks)\n",
    "- Conduct a comprehensive evaluation of the model's overall performance using **appropriate metrics** that account for the specific characteristics of the task\n",
    "- Your analysis must:\n",
    "  - Use **at least two different metrics** beyond basic ones you already used\n",
    "  - Justify why each selected metric is appropriate\n",
    "  - Calculate and report metric values on the validation set\n",
    "  - Contextualize what these metrics reveal about the model's capabilities and limitations\n",
    "  - Address both the regression (positional) and classification (gripper action) components separately\n",
    "\n",
    "**Example approach:** \"For gripper action classification, the -- of X indicates... considering ... . For positional predictions, the ...  of Y suggests...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 800 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 1.b.ii.iii}$ Systematic Failure Analysis (9 marks)\n",
    "##### Task\n",
    "- Move beyond overall metrics to investigate **systematic failure patterns** by examining specific data samples\n",
    "- You must:\n",
    "  - Identify and demonstrate **recurring failure modes** using concrete examples from the validation set\n",
    "  - Quantify the prevalence and impact of each failure mode (e.g., \"This pattern affects Y% of samples with characteristic Z\")\n",
    "  - Analyze **why** the model struggles with these specific types of observations\n",
    "  - Connect failure patterns to model architecture or data characteristics\n",
    "\n",
    "**Required approach:**\n",
    "- Examine actual predictions on misclassified validation samples\n",
    "- Look for patterns in the types of observations where the model consistently fails\n",
    "- Consider visual analysis of image inputs where failures occur\n",
    "- Analyze whether failures correlate with specific input characteristics (e.g., object positions, camera angles, gripper states)\n",
    "\n",
    "**IMPORTANT:**\n",
    "- Marks will be heavily penalized if you only discuss overall metrics without examining specific failure cases\n",
    "- You must demonstrate systematic patterns rather than isolated examples\n",
    "- If you find no systematic failure modes, you must provide comprehensive empirical evidence examining diverse data subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 900 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\color{Red}{Question\\ 1.c}$ Model tuning\n",
    "Now you have evaluated the proposed model, you are required to iterate and train a higher performing one. You are expected to run experiments that help you understand where the model is underperforming, guiding your development.\n",
    "\n",
    "#### $\\color{Red}{Question\\ 1.c.i}$ Model tuning (5 marks)\n",
    "##### Task\n",
    "- Using the code blocks below, implement a model which improves over the previous. Improve the performance as best you can and report the results using the metric/metrics you used in question 1.b.ii Model evaluation. Markers should be able to retrain your model by running the cell below. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 200 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 1.c.ii}$ Discussion (marks broken down in subsections below)\n",
    "\n",
    "##### Task\n",
    "In the markdown blocks below, discuss **three** experiments that you ran during the development of your model, defined in 1.c.i Model tuning which were the **most insightful** with respect to the design of your final model. Importantly, **most insightful** needn't necessarily be the final decisions that appear in the model defined in question 1.c.i Model tuning, rather they should be experiments that most shaped your model development process. For example: \n",
    "- You might decide to use a learning rate scheduler and decrease the learning rate at epoch 10 (call this experiment \"EXP_LR\");\n",
    "- This experiment produces a jump in performance and unlocks a series of further fruitful experiments into learning rate scheduling;\n",
    "- However, in the model reported in question 1.c.i Model tuning, you use a learning rate schedule with descreases at epochs 6 and 15 (since these produced a marginal performance over \"EXP_LR\")\n",
    "- For this question __you should discuss EXP_LR__.\n",
    "  \n",
    "For each experiment, complete the \"Description\", \"Result\" and \"Conclusion\" sections where the following information should be provided:\n",
    "* __Description__: What delta were you measuring i.e., change of architecture, change of learning rate etc?\n",
    "* __Justification__: **Why** are you conducting the experiment?\n",
    "  * What was the context of the model development process up to this point? What did you already know about how well the model performed/why it was performing/what were the challenging data points?\n",
    "* __Conclusion__: What did you __learn__ from the experiment and provide __empirical evidence__ to support this claim. In drawing your conclusions, consider where there are multiple possible causes for the model failing, ensure you provide evidence for each of these and conclude whether or not they might be root cause (you may conclude that there are multiple causes).\n",
    "\n",
    "**IMPORTANT** If your reported experiments are **not** well motivated and do not demonstrate that you have integrogated the model performance, you will be deducted marks. An example of poor motovation might be: *I chose to decrease the learning rate from X to Y as I did not know what impact this might have*. This justification could be improved by explaining what you __already know__ about how the learning rate might be affecting the model. What hypotheses might you draw about what the experiment will show?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{Red}{Question\\ 1.c.ii.i}$ Experiment 1 discussion (10 marks)\n",
    "##### Description:\n",
    "##### Justification:\n",
    "##### Conclusion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 800 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 1.c.ii.ii}$ Experiment 2 discussion (10 marks) \n",
    "\n",
    "##### Description:\n",
    "##### Justification:\n",
    "##### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 800 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 1.c.ii.iii}$ Experiment 3 discussion (10 marks) \n",
    "##### Description:\n",
    "##### Justification:\n",
    "##### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 800 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 Self-supervised pretraining with VAEs\n",
    "\n",
    "This question requires you to implement a self-supervised approach using a VAE architecture. The focus of question 2 is in developing a VAE model without using __any__ supervised information i.e., without using any action information. You will assess the convergence of the model and the suitability of the learnt latent space. You are required to select and appropriate architecture/loss function/target set of observations to perform self-supervised learning over.\n",
    "\n",
    "__IMPORTANT__: Do not use any of the __action__ information. You will be awarded 0 marks if you do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{Red}{Question\\ 2.a}$ Self-supervised VAE model (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Implement the full model training process and model definiton in the code block below. Markers should be able to retrain your model by running the cell below. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 200 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\color{Red}{Question\\ 2.b}$ Model convergence (12 marks)\n",
    "\n",
    "##### Task\n",
    "- Provide empirical evidence (in the form of appropriate training/validation metrics) supporting **why** the model is appropriately trained;\n",
    "- Interpret why the metrics demonstrate that the model has converged. If your model has **not** converged, interpret why the metrics suggest so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 900 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\color{Red}{Question\\ 2.c}$ Latent space analysis (6 marks)\n",
    "\n",
    "##### Task\n",
    "- Using reasonable analysis, conclude whether the representation learnt by the self-supervised method will be beneficial for the downstream supervised task. **Empirical evidence** should be provided **however**, references to the performance of the self-superised method with a supervised head on the downstream task will be awarded __0 marks__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 600 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{Red}{Question\\ 3}$ Full model training\n",
    "This question requires you to first develop a supervised head which utilises the latent space from your self-supervised method. You are then required to assess the models performance as well as the model you developed in question 1.c.i on the test set, report the performance and conclude whether self-supervised learning is appropriate for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{Red}{Question\\ 3.a}$ Combining self-supervised model with the downstream task (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Develop a model which combines the self-supervised pretraining with a model for performing the downstream task by freezing the self-supervised model and fine-tuning a head for prediction and implement it in the code block below. Markers should be able to retrain your model by running the cell below. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions). The supervised head should at least include any inputs that you did not feed into the self-supervised model. For example, assume you decide to perform self-supervised learning only using front_cam_ob images. You must also include mount_cam_ob, ee_cartesian_pos_ob, ee_cartesian_vel_ob and joint_pos_ob observations in your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 200 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### $\\color{Red}{Question\\ 3.b}$ Assessing the suitability of self-supervised learning \n",
    "For the final two questions, you are required to assess the performance of the self-supervised + supervised head model and the end-to-end model that you have trained. Additionally, you are required to holistically evaluate whether self-supervised learning has been beneficial for this task.\n",
    "\n",
    "#### $\\color{Red}{Question\\ 3.b.i}$ Assessing the suitability of self-supervised learning (4 marks) \n",
    "\n",
    "##### Task\n",
    "- In the code block below, evaluate the performance of the model you trained in question 3.a and the model you trained in question 1.c.i, using the test set. Additionally use the same metrics to train and evaluate the model that you used for question 1.c.i. Markers should be able to run the cell such that both models are run on the appropriate dataset. You may however, reference previously defined objects (e.g., loaded data and use previously defined functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 200 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### $\\color{Red}{Question\\ 3.b.ii}$ Justification (5 marks)\n",
    "\n",
    "##### Task\n",
    "- Conclude whether the self-supervised pre-training was beneficial for the task of predicting actions. Your answer should not solely focus on final performance but rather be nuianced and balance other model development considerations for example parameter count and speed of convergence. Also, if you believe the comparison between the model trained in question 3.a.i aganst the model trained in question 2.c.i is not _fair_, discuss further experiments which you would perform to reduce the bias in your conclusions. Provide __empirical evidence__ to support your conclusions. __0 marks__ will be awarded if empirical evidence is __not__ provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Response:}$ (Maximum 600 words in text)\n",
    "\n",
    "`[Pls use this markdown cell to provide your textual Reponse]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pls use this code cell to excute the code to generate your emprical evidence to support your textual response above\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE *******\n",
    "\n",
    "# ******* INSERT YOUR CODE HERE - END *******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - End of Coursework -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursework2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
